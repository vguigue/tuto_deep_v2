{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Séance 1 :  Deep Learning - Introduction à Pytorch \n",
    "\n",
    "Les notebooks sont très largement inspirés des cours de **N. Baskiotis et B. Piwowarski**. Ils peuvent être complétés efficacement par les tutoriels *officiels* présents sur le site de pytorch:\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "Au niveau de la configuration, toutes les installations doivent fonctionner sur Linux et Mac. Pour windows, ça peut marcher avec Anaconda à jour... Mais il est difficile de résoudre les problèmes.\n",
    "\n",
    "* Aide à la configuration des machines: [lien](https://dac.lip6.fr/master/environnement-deep/)\n",
    "* Alternative 1 à Windows: installer Ubuntu sous Windows:  [Ubuntu WSL](https://ubuntu.com/wsl)\n",
    "* Alternative 2: travailler sur Google Colab (il faut un compte gmail + prendre le temps de comprendre comment accéder à des fichers) [Colab](https://colab.research.google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaW5Av4elaBN"
   },
   "source": [
    "# A. Exemple typique de code complet & applications\n",
    "* Le graphe de calcul est instancié de manière dynamique sous pytorch, et cela consomme des ressources. Lorsqu'il n'y a pas de rétropropagation qui intervient - lors de l'évaluation d'un modèle par exemple -, il faut à tout prix éviter de le calculer. L'environnement **torch.no_grad()** permet de désactiver temporairement l'instanciation du graphe. **Toutes les procédures d'évaluation doivent se faire dans cet environnement afin d'économiser du temps !**\n",
    "* Pour certains modules, le comportement est différent entre l'évaluation et l'apprentissage (pour le dropout ou la batchnormalisation par exemple, ou pour les RNNs). Afin d'indiquer à pytorch dans quelle phase on se situe, deux méthodes sont disponibles dans la classe module,  **.train()** et **.eval()** qui permettent de basculer entre les deux environnements.\n",
    "\n",
    "Les deux fonctionalités sont très différentes : **no_grad** agit au niveau du graphe de calcul et désactive sa construction (comme si les variables avaient leur propriété **requires_grad** à False), alors que **eval/train** agissent au niveau du module et influence le comportement du module.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple typique de code pytorch qui reprend l'ensemble des éléments de ce tutoriel. Vous êtes prêt maintenant à expérimenter la puissance de ce framework.\n",
    "\n",
    "## A.1. Exemple complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3TRg2p5ldCJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# pour les MAC\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# outils avancés de gestion des chemins\n",
    "BASEPATH = Path(\"/tmp\")\n",
    "TB_PATH =  BASEPATH / \"logs\"\n",
    "TB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# usage externe de tensorboard: (1) lancer la commande dans une console; (2) copier-coller l'URL dans un navigateur\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")\n",
    "print(\"Une fois effectué, copier-coller l'URL dans votre navigateur pour avoir les courbes d'apprentissage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets: construction du jeu de données + séparation apprentissage / test\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing(data_home=\"./data/\") ## chargement des données\n",
    "all_data = torch.tensor(housing['data'],dtype=torch.float)\n",
    "all_labels = torch.tensor(housing['target'],dtype=torch.float)\n",
    "\n",
    "# Il est toujours bon de normaliser\n",
    "all_data = (all_data-all_data.mean(0))/all_data.std(0)\n",
    "all_labels = (all_labels-all_labels.mean())/all_labels.std()\n",
    "\n",
    "train_tensor_data = TensorDataset(all_data, all_labels)\n",
    "\n",
    "# Split en 80% apprentissage et 20% test\n",
    "train_size = int(0.8 * len(train_tensor_data))\n",
    "validate_size = len(train_tensor_data) - train_size\n",
    "train_data, valid_data = torch.utils.data.random_split(train_tensor_data, [train_size, validate_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model(fichier,model):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "      else:\n",
    "           print(\"Erreur de chargement du fichier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La boite suivante prend 3 ou 4 minutes\n",
    "* Utiliser TensorBoard pour suivre l'apprentissage\n",
    "* pas besoin d'aller au bout: ce qui nous intéresse, c'est l'architecture générale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(all_data.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "net.name = \"mon_premier_reseau\"\n",
    "net = net.to(device)\n",
    "MyLoss = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(params=net.parameters(),lr=1e-5)\n",
    "#optim = torch.optim.Adam(params=net.parameters(),lr=1e-3)\n",
    "\n",
    "# On créé un writer avec la date du modèle pour s'y retrouver\n",
    "summary = SummaryWriter(f\"{Path(TB_PATH)}/model-{time.asctime()}\".replace(\":\",\"_\"))\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Apprentissage\n",
    "    # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "    net.train()\n",
    "    cumloss = 0\n",
    "    for xbatch, ybatch in train_loader:\n",
    "        xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "        outputs = net(xbatch)\n",
    "        loss = MyLoss(outputs.view(-1),ybatch)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    summary.add_scalar(\"loss/train loss\",  cumloss/len(train_loader),epoch)\n",
    "     \n",
    "    if epoch % 10 == 0: \n",
    "        # Validation\n",
    "        # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            cumloss = 0\n",
    "            for xbatch, ybatch in valid_loader:\n",
    "                xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "                outputs = net(xbatch)\n",
    "                cumloss += MyLoss(outputs.view(-1),ybatch).item()\n",
    "            summary.add_scalar(\"loss/validation loss\", cumloss/len(valid_loader) ,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si vous êtes allé au bout de l'entrainement\n",
    "path = Path(\"./model\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "fichier = path/f\"{net.name}\"\n",
    "\n",
    "# ramener le réseau en mémoire avant la sauvegarde\n",
    "save_model(net.to(\"cpu\"),fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger un modèle déjà entrainé (attention, écrasement du réseau présent en mémoire)\n",
    "\n",
    "load_model(\"model/premier-reseau-pretrained\", net)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test de la performance du réseau chargé:\n",
    "with torch.no_grad():\n",
    "    cumloss = 0\n",
    "    for xbatch, ybatch in valid_loader:\n",
    "        xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "        outputs = net(xbatch)\n",
    "        loss = MyLoss(outputs.view(-1),ybatch)\n",
    "        cumloss += loss.item()\n",
    "print(\"loss: \",cumloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HujGOuB9lte2"
   },
   "source": [
    "## A.2. Jeu de données MNIST\n",
    "Ce jeu de données est l'équivalent du *Hello world* en programmation. Chaque donnée est un chiffre manuscrit (de 0 à 9). Les lignes suivantes vous permettent de charger le jeu de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH_GScQbltD0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "# Téléchargement des données\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dimension of images (flattened)\n",
    "HEIGHT,WIDTH = train_set[0][0].shape[1],train_set[0][0].shape[2] # taille de l'image\n",
    "INPUT_DIM = HEIGHT * WIDTH\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe arbitrairement la taille du mini batch à 32\n",
    "all_train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
    "all_test_loader = DataLoader(test_set,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alloOoFulri7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## Affichage de quelques chiffres\n",
    "ex,lab = next(iter(all_train_loader))\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(ex[i].view(WIDTH,HEIGHT), cmap='gray', interpolation='none')\n",
    "  plt.title(\"Label : {}\".format(lab[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  ax = plt.gca()\n",
    "  ax.set_facecolor('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjPoDcEj28uk"
   },
   "source": [
    "##  A.3. <span class=\"alert-success\"> Exercice : Classification multi-labels, nombre de couche de couches, fonction de coût </span>\n",
    "\n",
    "L'objectif est de classer chaque image parmi les 10 chiffres qu'ils représentent. Le réseau aura donc 10 sorties, une par classe, chacune représentant la probabilité d'appartenance à chaque classe. Pour garantir une distribution de probabilité en sortie, il faut utiliser le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html> **Softmax** </a> : $$Sotfmax(\\mathbf{x}) = \\frac{\\exp{x_i}}{\\sum_{i=1^d} x_i}$$ qui permet de normaliser le vecteur de sortie.\n",
    "\n",
    "* Faites quelques exemples de réseau à 1, 2, 3 couches et en faisant varier les nombre de neurones par couche. Utilisez un coût moindre carré dans un premier temps. Pour superviser ce coût, on doit construire le vecteur one-hot correspondant à la classe : un vecteur qui ne contient que des 0 sauf à l'index de la classe qui contient un 1 (utilisez ```torch.nn.functional.one_hot```).  Comparez les courbes de coût et d'erreurs en apprentissage et en test selon l'architecture.\n",
    "* Le coût privilégié en multi-classe est la *cross-entropy**. Ce coût représente la négative log-vraisemblance : $$NNL(y,\\mathbf{x}) = -x_{y}$$ en notant $y$ l'indice de la classe et $\\mathbf{x}$ le vecteur de log-probabilité inféré. On peut utiliser soit son implémentation par le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>**NLLLoss**</a>, soit - plus pratique - le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>**CrossEntropyLoss** <a>  qui combine un *logSoftmax* et la cross entropie, ce qui évite d'avoir à ajouter un module de *Softmax* en sortie du réseau. Utilisez ce dernier coût et observez les changements.\n",
    "* Changez la fonction d'activation en une ReLU et observez l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38Q58_1b-tQs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
    "TRAIN_RATIO = 0.01\n",
    "train_length = int(len(train_set)*TRAIN_RATIO)\n",
    "ds_train, ds_test = random_split(train_set, (train_length, len(train_set)- train_length))\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "\n",
    "def accuracy(yhat,y):\n",
    "    # si  y encode les indexes\n",
    "    if len(y.shape)==1 or y.size(1)==1:\n",
    "        return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()\n",
    "    # si y est encodé en onehot\n",
    "    return (torch.argmax(yhat,1).view(-1) == torch.argmax(y,1).view(-1)).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# On construit un réseau générique, où le nombre de couche pourra varier\n",
    "# 10 sorties pour chaque réseau, une par classe. \n",
    "# DANS la boucle d'apprentissage:\n",
    "#   SOL 1: cross entropy loss, 10 sorties, on ne rescale pas les sorties <=> GT = int\n",
    "#          (la cross entropy combine un softmax + NLLloss)\n",
    "#   SOL 2: MSE, 10 sorties + softmax <=> GT = 10 floats 0./1.\n",
    "\n",
    "# Etape 1: Réseau générique\n",
    "\n",
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self,in_features,out_features,dims=[16],activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        # liste des couches\n",
    "        layers = []\n",
    "        # Remplir la liste avec les bons modules, à la bonne taille, dans l'ordre\n",
    "        # [16] => [inputdim -> 16 -> outputdim]\n",
    "        # [32, 20, 16] => [inputdim -> 32 -> 20 -> 16 -> outputdim]\n",
    "        # avec à chaque fois les bonnes activation\n",
    "        ##  TODO \n",
    "        self.layers = nn.Sequential(*layers) # astuce pour créer le réseau à partir de la liste!\n",
    "\n",
    "    def forward(self,input):\n",
    "        return self.layers(input)\n",
    "        \n",
    "# Boucle typique pour l'entraînement intégrée dans une méthode\n",
    "def run(model,epochs,loss_type=\"MSE\"):\n",
    "    writer = SummaryWriter(f\"{Path(TB_PATH)}/{model.name}-{loss_type}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3) # gradient le plus classique/robuste\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"running {model.name}--{loss_type}\")\n",
    "\n",
    "    # SI loss_type == MSE\n",
    "    #   - definition de la loss : MSELoss\n",
    "    #   - transformation des sorties du réseau : transf_out = softmax \n",
    "    #   - transformation des étiquettes : transf_lab = one_hot + passage en float\n",
    "    # SINON\n",
    "    #   - definition de la loss : CrossEntropyLoss\n",
    "    #   - transfos = identité\n",
    "\n",
    "    if loss_type == \"MSE\":\n",
    "        loss = nn.MSELoss()\n",
    "        transf_out = nn.Softmax(dim=1)\n",
    "        transf_lab = lambda x: one_hot(x,10).float()\n",
    "    else:\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        transf_out = lambda x: x\n",
    "        transf_lab = lambda x: x\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), transf_lab(y).to(device) # usage des fonctions def ci-dessus\n",
    "            yhat = transf_out(model(x)) # usage des fonctions def ci-dessus\n",
    "            l = loss(yhat,y) # usage des fonctions def ci-dessus\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar(f'loss{loss_type}/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), transf_lab(y).to(device)\n",
    "                    yhat = transf_out(model(x))\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss{loss_type}/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on va tester 12 réseaux = 2 loss x 3 profondeurs x 2 nb de neurones cachés\n",
    "# on choisira une activation de type RELU\n",
    "\n",
    "# si vous êtes allé au bout de l'entrainement\n",
    "path = Path(\"./model\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for l in [\"MSE\",\"CEL\"]:\n",
    "    for nb_couches in [1,2,3]:\n",
    "        for nb_dim in [16,64]:\n",
    "\n",
    "            # Créer l'instance de réseau avec les bons paramètres\n",
    "            ##  TODO \n",
    "            net.name = f\"net-{nb_dim}-{nb_couches}\"+time.asctime().replace(\":\",\"_\")\n",
    "            run(net,2000,l) # apprentissage d'un réseau [dans une double boucle !]\n",
    "\n",
    "            # ramener le réseau en mémoire avant la sauvegarde\n",
    "            fichier = path/f\"{net.name}\"\n",
    "            save_model(net.to(\"cpu\"),fichier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4.  <span class=\"alert-success\"> Exercice : Régularisation des réseaux </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pénalisation des couches\n",
    "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser (cf code ci-dessous).\n",
    "\n",
    "Expérimentez avec une norme L2 dans $\\{0,10^{-5},10^{-4},10^{-3},10^{-2},\\}$, observez les histogrammes de la distribution des poids et l'évolution de la pénalisation et du coût en fonction du nombre d'époques. Utilisez pour cela  un réseau à 3 couches chacune de taille 100 et un coût de CrossEntropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_l2(model,epochs,l2_coef):\n",
    "    writer = SummaryWriter(f\"/tmp/logs/l2-{l2_coef}-{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}-{l2_coef}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "\n",
    "            # Ajout d'une pénalisation L2 sur toutes les couches\n",
    "            l2_loss = 0.\n",
    "            for name, value in model.named_parameters():\n",
    "                if name.endswith(\".weight\"):\n",
    "                    l2_loss += (value ** 2).sum()\n",
    "            l += l2_coef*l2_loss\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "            \n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        writer.add_scalar('loss/l2',l2_loss,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "                ix = 0\n",
    "                for module in model.layers:\n",
    "                    if isinstance(module, nn.Linear):\n",
    "                        writer.add_histogram(f'linear/{ix}/weight',module.weight, epoch)\n",
    "                        ix += 1\n",
    "\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau.\n",
    "\n",
    "Le comportement du réseau est donc différent en apprentissage et en inférence. Il est obligatoire d'utiliser ```model.train()``` et ```model.eval()``` pour différencier les comportements.\n",
    "Testez sur quelques réseaux pour voir l'effet du dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQsK7jHuxKqM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_dropout(model,epochs):\n",
    "    writer = SummaryWriter(f\"{Path(TB_PATH)}/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "\n",
    "\n",
    "def get_dropout_net(in_features,out_features,dims,dropout):\n",
    "    layers = []\n",
    "    dim = in_features\n",
    "    \n",
    "    for newdim in dims:\n",
    "        layers.append(nn.Linear(dim, newdim))\n",
    "        dim = newdim\n",
    "        if dropout>0: layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.ReLU())\n",
    "        dim = newdim\n",
    "    layers.append(nn.Linear(dim,out_features))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm\n",
    "\n",
    "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
    "apprentissage).\n",
    "\n",
    "Tout comme pour le dropout, il est nécessaire d'utiliser ```model.train()``` et ```model.eval()```. \n",
    "Expérimentez la batchnorm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batchnorm_net(in_features,out_features,dims):\n",
    "    layers = []\n",
    "    dim = in_features\n",
    "    for newdim in dims:\n",
    "        layers.append(nn.Linear(dim, newdim))\n",
    "        dim = newdim\n",
    "        layers.append(nn.BatchNorm1d(dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        dim = newdim\n",
    "    layers.append(nn.Linear(dim,out_features))\n",
    "    return nn.Sequential(*layers)\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
