{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Séance 1 :  Deep Learning - Introduction à Pytorch \n",
    "\n",
    "Les notebooks sont très largement inspirés des cours de **N. Baskiotis et B. Piwowarski**. Ils peuvent être complétés efficacement par les tutoriels *officiels* présents sur le site de pytorch:\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "Au niveau de la configuration, toutes les installations doivent fonctionner sur Linux et Mac. Pour windows, ça peut marcher avec Anaconda à jour... Mais il est difficile de récupérer les problèmes.\n",
    "\n",
    "* Aide à la configuration des machines: [lien](https://dac.lip6.fr/master/environnement-deep/)\n",
    "* Alternative 1 à Windows: installer Ubuntu sous Windows:  [Ubuntu WSL](https://ubuntu.com/wsl)\n",
    "* Alternative 2: travailler sur Google Colab (il faut un compte gmail + prendre le temps de comprendre comment accéder à des fichers) [Colab](https://colab.research.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Y9YOOHHhJKY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.2.2\n",
      "Le calcul GPU est disponible ?  False\n",
      "Le calcul GPU est disponible (apple) ?  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "# pour les possesseurs de mac M1 avec la dernière version de pytorch:\n",
    "print(\"Le calcul GPU est disponible (apple) ? \", torch.backends.mps.is_available())\n",
    "\n",
    "# activation plus tard dans les TP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chargement des données housing (depuis sklearn) et transformation en tensor.\n",
    "# from sklearn.datasets import load_housing # => removed\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing(data_home=\"./data/\") ## chargement des données\n",
    "\n",
    "housing_x = torch.tensor(housing['data'],dtype=torch.float) # penser à typer les données pour éliminer les incertitudes\n",
    "housing_y = torch.tensor(housing['target'],dtype=torch.float)\n",
    "\n",
    "print(\"Nombre d'exemples : \",housing_x.size(0), \"Dimension : \",housing_x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(housing['feature_names']))\n",
    "\n",
    "# affichage des 5 premiers éléments\n",
    "print(housing_x[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1rxv3ychdhD"
   },
   "source": [
    "## A. Architecture modulaires & réseaux de neurones\n",
    "Dans le framework pytorch (et dans la plupart des frameworks analogues), le module est la brique de base qui permet de construire un réseau de neurones.  Il permet de représenter en particulier :\n",
    "* une couche du réseau (linéaire : **torch.nn.Linear**, convolution : **torch.nn.convXd**, ...)\n",
    "* une fonction d'activation (tanh : **torch.nn.Tanh**, sigmoïde : **torch.nn.Sigmoid** , ReLu : **torch.nn.ReLu**, ...)\n",
    "* une fonction de coût (MSE : **torch.nn.MSELoss**, L1 :  **torch.nn.L1Loss**, CrossEntropy binaire: **torch.BCE**, CrossEntropy : **torch.nn.CrossEntropyLoss**, ...)\n",
    "* un ensemble de modules : en termes informatique, un module est un conteneur abstrait qui peut contenir d'autres conteneurs) : plusieurs modules peuvent être mis ensemble afin de former un nouveau module plus complexe.\n",
    "\n",
    "\n",
    "Le fonctionnement est très proche des fonctions que nous avons vu ci-dessus (un module encapsule en fait une fonction de **torch.nn.Function**), mais de manière à gérer automatiquement les paramètres à apprendre. Un module est ainsi muni :\n",
    "* d'une méthode **forward** qui permet de calculer la sortie du module à partir des entrées\n",
    "* d'une méthode **backward** qui permet d'effectuer la rétro-propagation (localement).\n",
    "* tous les paramètres sont automatiquement ajoutés dans une liste interne, accessible par la fonction **.parameters()** du module.\n",
    "\n",
    "Ci-dessous un exemple de régression linéaire en utilisant les modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spguRLUjD60C"
   },
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "EPS = 1e-7\n",
    "\n",
    "Xdim = housing_x.size(1)\n",
    "## Création d'une couche linéaire de dimension Xdim->1\n",
    "net = torch.nn.Linear(Xdim, 1) \n",
    "\n",
    "## Passe forward du module :  équivalent à net.forward(x)[:10]\n",
    "print(\"Sortie du réseau\", net(housing_x)[:10])\n",
    "## affiche la liste des paramètres du modèle\n",
    "print(\"Paramètres et noms des paramètres\", list(zip(list(net.parameters()), list(net.named_parameters()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Création d'une fonction de loss aux moindres carrés\n",
    "mseloss = torch.nn.MSELoss()\n",
    "## on créé un optimiseur pour le réseau (paramètres w et b), avec un pas de gradient lr\n",
    "optim = torch.optim.SGD(params=net.parameters(),lr=EPS) \n",
    "# Juste pour info, ce n'est pas utile, les paramètres sont déjà initialisés.\n",
    "net.reset_parameters()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss = mseloss(net(housing_x).view(-1,1),housing_y.view(-1,1))\n",
    "    print(f\"iteration : {i}, loss : {loss}\")\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxdRGqZtSlVt"
   },
   "source": [
    "## A.1. Création d'un réseau de neurones\n",
    "\n",
    "Avec ces briques élémentaires, il est très facile de définir un réseau de neurones standard :\n",
    "* soit en utilisant le conteneur **torch.nn.Sequential** qui permet d'enchaîner séquentiellement plusieurs modules\n",
    "* soit en définissant à la main un nouveau module.\n",
    "\n",
    "Ci-dessous un exemple  pour créer un réseau à deux couches linéaires avec une fonction d'activation tanh des deux manières différentes. Vous remarquez qu'il n'y a pas besoin de définir la méthode **backward**, celle-ci est héritée du conteneur abstrait et ne fait qu'appeler séquentiellement en ordre inverse les méthodes **backward** des différents modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c6I-PZRD-aN"
   },
   "outputs": [],
   "source": [
    "EPS = 1e-2\n",
    "EPOCHS=50\n",
    "\n",
    "#Réseau à la main (on le refera à la main derriere)\n",
    "class DeuxCouches(torch.nn.Module):\n",
    "  def __init__(self, Xdim):\n",
    "    super(DeuxCouches,self).__init__()\n",
    "    self.un = torch.nn.Linear(Xdim,5)\n",
    "    self.act = torch.nn.Tanh()\n",
    "    self.deux = torch.nn.Linear(5,1)\n",
    "  def forward(self,x):\n",
    "    return self.deux(self.act(self.un(x)))\n",
    "\n",
    "netDeuxCouches = DeuxCouches(Xdim)\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "    \n",
    "optim = torch.optim.SGD(params=netDeuxCouches.parameters(),lr=EPS)\n",
    "for i in range(EPOCHS):\n",
    "    loss = mseloss(netDeuxCouches(housing_x),housing_y.view(-1,1))\n",
    "    print(f\"iteration : {i}, loss : {loss}\")\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un réseau à 1 couche cachée avec le module séquentiel (remplace l'objet précédent)\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "mseloss = torch.nn.MSELoss()    \n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS) # extraction auto des paramètres :)\n",
    "for i in range(EPOCHS):\n",
    "    loss = mseloss(netSeq(housing_x),housing_y.view(-1,1))\n",
    "    print(f\"iteration : {i}, loss : {loss}\")\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ4MoJP4k4i0"
   },
   "source": [
    "## B. Outils annexes\n",
    "\n",
    "### B.0 tdqm\n",
    "Afin de rendre les boucles `for` plus élégantes et surtout plus *suivable*, il faut utiliser le package `tdqm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avant tdqm\n",
    "import time\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    time.sleep(np.random.rand())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# après, avec tdqm\n",
    "from tqdm import tqdm \n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    # print(i)\n",
    "    time.sleep(np.random.rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### B.1. DataLoader\n",
    "\n",
    "Pytorch dispose d'un ensemble d'outils qui permettent de simplifier les démarches expérimentales. Nous allons voir en particulier en commençant par\n",
    "* le DataLoader qui permet de gérer le chargement de données, le partitionement et la constitution d'ensembles de test et d'apprentissage; \n",
    "\n",
    "Le <a href=https://pytorch.org/docs/stable/data.html>**DataLoader**</a> et la classe associée <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset> **Dataset**</a>  permettent en particulier de :\n",
    "* charger des données\n",
    "* pré-processer les données\n",
    "* de gérer les mini-batchs (sous-ensembles sur lequel on effectue une descente de gradient).\n",
    "\n",
    "La classe **Dataset** est une classe abstraite qui nécessite l'implémentation que d'une seule méthode, ```__getitem__(self,index)``` : elle renvoie le i-ème objet du jeu de données (généralement un couple *(exemple,label)*. \n",
    "\n",
    "La classe **TensorDataset** est l'instanciation la plus courante d'un **Dataset**, elle permet de créer un objet **Dataset** à partir d'une liste de tenseurs qui renvoie pour un index $i$ donné le tuple contenant les $i$-èmes ligne de chaque tenseur.\n",
    "\n",
    "La classe **DataLoader** permet essentiellement de randomiser et de constituer des mini-batchs de façon simple à partir d'une instance de **Dataset**. Chaque mini-batch est constitué d'exemples tirés aléatoirement dans le **Dataset** passé en paramètre et mis bout à bout dans des tenseurs. La méthode ```collate_fn(*args)``` est utilisée pour cela (nous verrons une customization de cette fonction dans une séance ultérieure). C'est ce générateur qui est généralement parcouru lors de l'apprentissage à chaque itération d'optimisation.\n",
    "\n",
    "Voici un exemple de code pour utiliser le DataLoader : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZaWAFO8k8ze"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "## Création d'un dataset à partir des deux tenseurs d'exemples et de labels\n",
    "train_data = TensorDataset(housing_x,housing_y)\n",
    "## On peut indexer et connaitre la longueur d'un dataset\n",
    "print(\"DATASET:\\n\",len(train_data),train_data[5])\n",
    "\n",
    "## Création d'un DataLoader\n",
    "## tailles de mini-batch de 16, shuffle=True permet de mélanger les exemples\n",
    "# loader est un itérateur sur les mini-batchs des données\n",
    "loader = DataLoader(train_data, batch_size=16,shuffle=True ) # n'hésitez pas à jouer avec les paramètres\n",
    "\n",
    "#Premier batch (aléatoire) du dataloader : (nb batch = len/batch_size)\n",
    "print(\"DATA LOADER:\\n\",len(iter(loader)),\"\\n\",next(iter(loader))[0].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpréter les dimensions issues du data loader pour être sur que le processus est compris en profondeur\n",
    "\n",
    "* Combien de fois chaque point est-il vu?\n",
    "* En combien de calcul traite-t-on l'ensemble des données?\n",
    "* Ecrire au brouillon l'expression littérale développée de `cumloss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "EPS=1e-4\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "# La boucle d'apprentissage :\n",
    "loop = tqdm(range(EPOCHS))\n",
    "for i in loop:\n",
    "    cumloss = 0\n",
    "    # On parcourt tous les exemples par batch de 16 (paramètre batch_size de DataLoader)\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item() # item pour un scalaire (sinon .data ou detach)\n",
    "    # old way\n",
    "    # print(f\"iteration : {i}, loss : {cumloss/len(loader)}\") # loss sur un batch => diviser pour avoir une grandeur interprétable\n",
    "    # new way with tqdm => On ajoute le message dans la barre de progression\n",
    "    loop.set_description(f\"loss : {cumloss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9x2LC_6lCQm"
   },
   "source": [
    "### B.2 Checkpointing\n",
    "Les modèles Deep sont généralement long à apprendre. Afin de ne pas perdre des résultats en cours de calcul, il est fortement recommander de faire du **checkpointing**, c'est-à-dire d'enregistrer des points d'étapes du modèle en cours d'apprentissage pour pouvoir reprendre à n'importe quel moment l'apprentissage du modèle en cas de problème.  Il s'agit en pratique de sauvegarder l'état du modèle et de l'optimisateur (et de tout autre objet qui peut servir lors de l'apprentissage) toutes les n itérations. Toutes les variables d'intérêt sont en général disponibles par la méthode **state_dict()** des modèles et de l'optimiseur. \n",
    "\n",
    "En pratique, vous pouvez utilisé un code dérivé de celui ci-dessous.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URQTq8hrPJO0"
   },
   "outputs": [],
   "source": [
    "# Il existe différentes solutions: en voici une\n",
    "# mais ça marche\n",
    "# \n",
    "import os\n",
    "\n",
    "def save_state(epoch,model,optim,fichier):\n",
    "      \"\"\" sauvegarde du modèle et de l'état de l'optimiseur dans fichier \"\"\"\n",
    "      state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_state(fichier,model,optim):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle et l'optimiseur \"\"\"\n",
    "      epoch = 0\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          optim.load_state_dict(state['optim_state'])\n",
    "          epoch = state['epoch']\n",
    "\n",
    "      return epoch\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter 2 fois les boites ci-dessous:\n",
    "* Première itération = 50 epochs\n",
    "* Deuxième itéation... Anticiper le nombre d'époch !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# construction rapide d'un réseau de neurones de type PMC\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS) # extraction auto des paramètres\n",
    "\n",
    "fichier = \"/tmp/netSeq.pth\"\n",
    "start_epoch = load_state(fichier,netSeq,optim)\n",
    "print(start_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in tqdm(range(start_epoch,EPOCHS)):\n",
    "    cumloss = 0\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if epoch % 10 ==0: save_state(epoch,netSeq,optim,fichier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IstQCvKblSvT"
   },
   "source": [
    "\n",
    "### B.3 GPU \n",
    "Afin d'utiliser un GPU lors des calculs, il est nécessaire de transférer les données et le modèle sur le GPU par l'intermédiaire de la fonction **to(device)** des tenseurs et des modules.  Il est impossible de faire une opération lorsqu'une partie des tenseurs sont sur GPU et l'autre sur CPU. Il faut que tous les tenseurs et paramètres soient sur le même device ! On doit donc s'assurer que le modèle, les exemples et les labels sont sur GPU pour faire les opérations.\n",
    "\n",
    "Par ailleurs, on peut connaître le device sur lequel est chargé un tenseur par l'intermédiaire de ```.device``` (mais pas pour un modèle, il faut aller voir les paramètres dans ce cas).\n",
    "\n",
    "Une manière simple d'utiliser un GPU quand il existe et donc d'avoir un code agnostique est la suivante : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fs8s7EwwlWTn"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# poru les possesseur de mac MXX:\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "## On charge le modèle sur GPU\n",
    "## A faire avant la déclaration de l'optimiseur, sinon les paramètres optimisés ne seront pas les mêmes! \n",
    "## model =  model.to(device) \n",
    "loader = DataLoader(TensorDataset(housing_x,housing_y), batch_size=16,shuffle=True ) \n",
    "\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netSeq = netSeq.to(device)\n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "for i,(bx,by) in tqdm(enumerate(loader)):\n",
    "    ## On charge le batch sur GPU\n",
    "    bx, by = bx.to(device), by.to(device)\n",
    "    loss = mseloss(netSeq(bx).view(-1),by)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # if i % 10 ==0: print(\"batch \",i)\n",
    "\n",
    "\n",
    "print(\"Device du mini-batch : \", bx.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5J1b55_lFR-"
   },
   "source": [
    "\n",
    "### B.4 TensorBoard\n",
    "\n",
    "Durant l'apprentissage de vos modèles, il est agréable de visualiser de quelle manière évolue le coût, la précision sur l'ensemble de validation ainsi que d'autres éléments. TensorFlow dispose d'un outil très apprécié, le TensorBoard, qui permet de gérer très facilement de tels affichages. On retrouve tensorboard dans **Pytorch** dans ```torch.utils.tensorboard``` qui permet de faire le pont de pytorch vers cet outil. \n",
    "\n",
    "Le principe est le suivant :\n",
    "* tensorboard fait tourner en fait un serveur web local qui va lire les fichiers de log dans un répertoire local. L'affichage se fait dans votre navigateur à partir d'un lien fourni lors du lancement de tensorboard.\n",
    "* Les éléments que vous souhaitez visualiser (scalaire, graphes, distributions, histogrammes) sont écrits dans le fichier de log à partir d'un objet **SummaryWriter** .\n",
    "* la méthode ```add_scalar(tag, valeur, global_step)``` permet de logger une valeur à un step donné, ```add_scalar(tag, tag_scalar_dic, global_step)``` un ensemble de valeurs par l'intermédiaire du dictionnaire ```tag_scalar_dic``` (un regroupement des scalaires est fait en fonction du tag passé, chaque sous-tag séparé par un **/**).\n",
    "\n",
    "Il existe d'autres méthodes ```add_XXX``` pour visualiser par exemple des images, des histogrammes (cf <a href=https://pytorch.org/docs/stable/tensorboard.html>la doc </a>).\n",
    "\n",
    "Le code suivant illustre une manière de l'utiliser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION 1: lancer tensorboard, ecrire des loss (ou autres indicateurs) dans un fichiers + lancer la visu dans dans un navigateur à part\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "\n",
    "# outils avancés de gestion des chemins\n",
    "BASEPATH = Path(\"/tmp\")\n",
    "TB_PATH =  BASEPATH / \"logs\"\n",
    "TB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# usage externe de tensorboard: (1) lancer la commande dans une console; (2) copier-coller l'URL dans un navigateur\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")\n",
    "print(\"Une fois effectué, copier-coller l'URL dans votre navigateur pour avoir les courbes d'apprentissage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kIhHDnElQd8"
   },
   "outputs": [],
   "source": [
    "# SOLUTION 2: Tensorbord à l'interieur du notebook (simple mais pas le plus pratique)\n",
    "\n",
    "# Solution moins intéressante: ne décommenter qu'en cas d'échec de la première solution !!\n",
    "\n",
    "\n",
    "# # Spécial notebook, les commandes suivantes permettent de lancer tensorboard\n",
    "# # En dehors du notebook, il faut le lancer à la main dans le shell : \n",
    "# # tensorboard --logdir logs\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /tmp/logs\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Spécial notebook : pour avoir les courbes qui s'affichent dans le notebook, \n",
    "# # sinon aller à l'adresse web local indiquée lors du lancement de tensorboard\n",
    "# from tensorboard import notebook\n",
    "# notebook.display() # A voir si vous avez une autre fenêtre de gestion de tensorboard ou si vous le voulez à la suite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPS = 1e-5\n",
    "EPOCHS=10\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netDeuxCouches = DeuxCouches(Xdim)\n",
    "netSeq.name = \"Sequentiel\" # nommer les modèles\n",
    "netDeuxCouches.name = \"DeuxCouches\"\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "for model in [netSeq, netDeuxCouches]:\n",
    "    ## Obtention d'un SummaryWriter\n",
    "    ## meme répertoire que la commande %tensorboard --logdir logs \n",
    "    # ATTENTION AUX noms de fichier sous windows:\n",
    "    # summary = SummaryWriter(f\"/tmp/logs/test/{model.name}/\".replace(\":\",\"_\"))\n",
    "    summary = SummaryWriter(f\"/tmp/logs/test/{model.name}/\") # on peut ajouter un timestamp ou des paramètres\n",
    "\n",
    "    optim = torch.optim.SGD(params=model.parameters(),lr=EPS) \n",
    "    for i in tqdm(range(EPOCHS)):\n",
    "        cumloss = 0\n",
    "        for bx, by in loader:\n",
    "            loss = mseloss(model(housing_x),housing_y.view(-1,1))\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()  \n",
    "            cumloss+= loss.item()\n",
    "        summary.add_scalar(f\"loss\",cumloss,i) # c'est ici qu'on fait le lien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.5 Les fonctions lambda (et l'encodage one-hot)\n",
    "\n",
    "2 choses distinctes: \n",
    "\n",
    "1. Les fonctions `lambda` sont très courantes en python, il faut *a minima* savoir les lire\n",
    "1. L'encodage one-hot est un outil récurrent dans tout le machine-learning... Et le deep-learning\n",
    "\n",
    "Ex: je veux passer d'un codage de classe en entiers : `[0, 1, 2]` <BR>\n",
    "à un codage en vecteur: `cl0 = [1, 0, 0]; cl1=[0, 1, 0]; cl2 = [0, 0, 1]`<BR>\n",
    "voire à un codage en réels `cl0 = [1., 0., 0.]; cl1=[0., 1., 0.]; cl2 = [0., 0., 1.]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda: une nouvelle façon de définir une fonction à la volée\n",
    "\n",
    "# 1. Construire la fonction 2 x^2 + 4\n",
    "monpoly = lambda x : 2 * x**2 + 4\n",
    "\n",
    "# 2. Utiliser la fonction\n",
    "x = [1., 2., 3.]\n",
    "print([(i, monpoly(i)) for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "Une erreur s'est produite : one_hot(): argument 'input' (position 1) must be Tensor, not int\n"
     ]
    }
   ],
   "source": [
    "# 3. Autre exemple pour transformer un entier en vecteur one-hot de dimension 3\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "to_one_hot = lambda x: one_hot(x,3).float()\n",
    "\n",
    "# on est dans torch => il faut donner des structures torch\n",
    "print(to_one_hot(torch.tensor(0)))\n",
    "print(to_one_hot(torch.tensor(1)))\n",
    "\n",
    "# ATTENTION aux types: le code suivant provoque une erreur\n",
    "try:\n",
    "    print(to_one_hot(2))\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# => Les fonctions torch ne marchent que sur des structures torch \n",
    "# contrairement aux fonctions numpy qui marchent (de plus en plus) sur des listes python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
